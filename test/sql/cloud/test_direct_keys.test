# name: test/sql/cloud/test_direct_keys.test
# description: test integration with iceberg catalog read
# group: [iceberg]

# require-env ICEBERG_SERVER_AVAILABLE
#
# require-env AWS_DEFAULT_REGION
#
# require-env AWS_ACCESS_KEY_ID
#
# require-env AWS_SECRET_ACCESS_KEY

require parquet

require iceberg

require httpfs

require aws

# test using keys directory
statement ok
CREATE SECRET s1 (
      TYPE S3,
      KEY_ID '${AWS_ACCESS_KEY_ID}',
      SECRET '${AWS_SECRET_ACCESS_KEY_ID}',
      REGION 'us-east-1'
);

statement ok
attach '840140254803:s3tablescatalog/pyiceberg-blog-bucket' as my_datalake (
    TYPE ICEBERG,
    ENDPOINT_TYPE 'GLUE'
);

query T nosort tables_1
show all tables;
----

statement ok
SELECT count(*) FROM my_datalake.myblognamespace.lineitem;

statement ok
drop secret s1;

statement ok
detach my_datalake;

# test using assume role
statement ok
CREATE SECRET assume_role_secret (
    TYPE S3,
    PROVIDER credential_chain,
    CHAIN 'sts',
    ASSUME_ROLE_ARN 'arn:aws:iam::840140254803:role/pyiceberg-etl-role',
    REGION 'us-east-1'
);

statement ok
attach '840140254803:s3tablescatalog/pyiceberg-blog-bucket' as my_datalake (
    TYPE ICEBERG,
    ENDPOINT_TYPE 'GLUE'
);

query T nosort tables_1
show all tables;
----

statement ok
select count(*) from my_datalake.myblognamespace.lineitem;

statement ok
drop secret assume_role_secret;

statement ok
detach my_datalake

# test using provider credential chain
statement ok
CREATE SECRET (
  TYPE S3,
  PROVIDER credential_chain
);

statement ok
attach '840140254803:s3tablescatalog/pyiceberg-blog-bucket' as my_datalake (
    TYPE ICEBERG,
    ENDPOINT_TYPE 'GLUE'
);

query T nosort tables_1
show all tables;
----
